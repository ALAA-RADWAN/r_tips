---
title: "Working with dummy variables and factors"
author: Erika Duan
date: "`r Sys.Date()`"
output:
  github_document:
    toc: true
---

```{r, message = FALSE, warning = FALSE}  
#-----load required packages-----  
if (!require("pacman")) install.packages("pacman")
pacman::p_load(here,
               tidyverse,
               fastDummies,
               caret,
               randomForest,
               xgboost,
               kernlab, 
               doParallel) # for creating dummy variables
```


# Introduction   

I found myself wrangling (or getting stuck on) factor levels for a good 30+ minutes earlier this week. This circumstance arose from different machine learning models requiring different data formats.   

Let's say you have a binary classification problem and you want to train a random forest model, a gradient boosted tree (XGBoost) model and a support vector model (SVM) model.  

**Requirements:**  

+ The random forest algorithm handles both numerical categorical and numeric predictor variables. The response variable needs to be a factor.     

+ The XGBoost algorithm only handles numerical predictor variables. The response variable needs to be converted into a numerical vector (i.e. `0` or `1`).    

+ The linear SVM algorithm requires scaled numerical predictor variables. The response variable needs to be converted into a factor that is not represented by `0` and `1`.           

This means that a single dataset can end up requiring multiple factor and dummy variable transformations. Let's explore this scenario further using a fun test dataset.      


# Creating a test dataset   

Cats are such idiosyncratic creatures. You can never really tell if a cat wants a pat but then decides to bite you instead. So what if machine learning could help us better predict cat behaviour (*to pat or not to pat - that is the question*)?      

**Note:** The code used to create this dataset can be accessed from the `Rmd` file accompanying this tutorial.   

```{r, echo = FALSE}
#-----creating a test dataset-----
# this chunk of code has no other purpose except to provide us with an example dataset    

set.seed(111) 

#-----create a vector of random cat breeds-----
cat_breed <- sample(c("british_shorthair",
                      "ragdoll",
                      "bengal",
                      "siamese",
                      "aristocat",
                      "mixed"),
                    size = 500,
                    replace = T)

#-----create a vector for age (rounded to 1 decimal point)-----  
age <- runif(500, 0.5, 9)  
age <- round(age, digits = 1)  

#-----create a vector for corresponding favourite activities-----
fav_activity <- sample(c("window_watching",
                         "hunting_toys",
                         "sitting_on_humans",
                         "napping"),
                       size = 500,
                       prob = c(0.10,
                                0.15,
                                0.25,
                                0.50),
                    replace = T)

#-----create a vector for likes_children-----  
likes_children <- sample(c("yes",
                           "no"),
                         size = 500,
                         prob = c(0.15, 0.85),
                         replace = T)

#-----creating the response variable-----
will_bite <- sample(c("yes", "no"),
                   size = 500,
                   prob = c(0.65, 0.35),
                   replace = T) 

#-----reconstruct two relationship in the dataset-----
cat_prediction <- tibble(cat_breed,
                         age,
                         fav_activity,
                         likes_children,
                         will_bite) 

cat_prediction <- cat_prediction %>%
  mutate(will_bite = case_when(likes_children == "yes" ~ "no",
                               TRUE ~ will_bite),
         will_bite = case_when(cat_breed %in% c("ragdoll", "siamese") ~ "no",
                               TRUE ~ will_bite))
```

```{r}
#-----using kable to quickly visualise the test dataset-----  
cat_prediction %>%
  head(10) %>%
  knitr::kable()
```

# Working with factors


## Creating factors  

There are two ways to handle factors:  

+ Using base `R` to create factors and modify factor levels.    
+ Using [`forcats`](https://r4ds.had.co.nz/factors.html) from the `tidyverse` library to simplify complex factor wrangling.      

Using `factor` without specifying factor levels automatically converts character vectors into a numerical index of character vectors. Specifying factor levels also allows you to specify the order of levels.  

**Note:** Factor levels need to match `unique(vector_values)` to prevent the generation of `NA` values.     

```{r}
#-----converting the response variable into a factor using base R-----  
cat_prediction_factor <- factor(cat_prediction$will_bite)

# str(cat_prediction$will_bite)
#> Factor w/ 2 levels "no","yes": 1 1 2 1 2 2 1 2 2 1 ...  

cat_prediction_factor <- factor(cat_prediction$will_bite,
                                   levels = c("no", "yes")) 

# make sure levels == unique(cat_prediction$will_bite)
# otherwise your response variable will be converted into NAs

cat_prediction_char <- as.character(cat_prediction_factor)  

# str(cat_prediction_char)
#> chr [1:500] "no" "no" "yes" "no" "yes" "yes" "no" "yes" "yes" "no" ...
```

Applying `as.character` back onto a factor returns the original character vector, rather than the numerical index.    

## Modifying factor levels  

In base R, modifying factor levels directly modifies the factor itself. I personally think that the safest way is to modify factor levels is by specifying replacement and original level names using `list`. Note that this approach requires all levels to be specified inside the list, or `NA` values will be generated.       

```{r}
#-----converting factor levels using base R-----
levels(cat_prediction_factor) <- list(no_biting = "no",
                                      yes_ouch = "yes")

# str(cat_prediction_factor)
#> Factor w/ 2 levels "no_biting","yes_ouch": 1 1 2 1 2 2 1 2 2 1 ...

cat_prediction_char <- as.character(cat_prediction_factor)

# str(cat_prediction_char)
#>  chr [1:500] "no_biting" "no_biting" "yes_ouch" "no_biting" "yes_ouch" ...

#----the dangers of modifying levels without referencing names----- 
cat_prediction_factor <- factor(cat_prediction$will_bite,
                                   levels = c("no", "yes")) 
 
# str(cat_prediction_factor)
#> Factor w/ 2 levels "no","yes": 1 1 2 1 2 2 1 2 2 1 ...

levels(cat_prediction_factor) <- c("yes", "no") # accidentally inverting factor level order
 
# str(cat_prediction_factor)
#> Factor w/ 2 levels "yes","no": 1 1 2 1 2 2 1 2 2 1 ...
```

You can also modify factor levels using [`fct_recode`](https://r4ds.had.co.nz/factors.html#modifying-factor-levels
) from `forcats`. The chief advantages of `fct_recode` are that:    

+ It does not modify levels that are not referenced (instead of converting them to `NA`).  
+ It will warn you if you accidentally refer to a level that does not exist.    

```{r, results = 'hide'}
#-----converting factor levels using forcats-----   
cat_prediction %>%
  mutate(will_bite_factor = factor(will_bite),
         will_bite_factor = fct_recode(will_bite,
                                       "no_biting" = "no",
                                       "yes_ouch" = "yes")) # new level name = old level name
```

**Note:** The `forcats` package also contains other functions for modifying or ordering factor levels, which are very useful for plotting graphs of categorical variables. This tutorial is limited to applying factors to binary data.   


# Using dummy variables  

## Creating dummy variables    

According to [Wikipedia](https://en.wikipedia.org/wiki/Dummy_variable_(statistics)),  a dummy variable is one that takes only the value 0 or 1 to indicate the absence or presence of some categorical effect that may be expected to shift the outcome. Note that if you have an ordinal variable with more than two responses, conversion into numerical values (`1`, `2`, `3` etc.) is **only valid** if the distance between each ordinal variable is exactly the same.    

I like to use the `fastDummies` package for fast conversion of character into binary (i.e. dummy) columns.     

```{r}
#-----using fastDummies::dummy_cols to create dummy variables-----  
# unique(cat_prediction$cat_breed) 

#> [1] "mixed"             "bengal"            "siamese"           "british_shorthair" "aristocat"        
#> [6] "ragdoll" 

# identify all categorial variables  

categorical_variables <- cat_prediction %>%
  select(-will_bite) %>% # remove response variable
  select_if(is.character) %>%
  colnames(.)

# create dummy variables
cat_prediction_num <- dummy_cols(cat_prediction, # .data
                                 select_columns = categorical_variables, 
                                 remove_selected_columns = T) # remove original categorical columns 

head(cat_prediction_num)
```


# Machine learning exercise  

Let's return to our original task of trying to use machine learning to reduce the risk of future unfriendly cat encouters. We want to take a random portion of our dataset and, using cross-validation, train a random forest, XGBoost and linear SVM model.    

## Generate train and test data     

As this is a simple dataset, we can use ``tidyverse` or `createDataPartition` from the `caret` library to create our train and test data.  

```{r}
#-----using caret::createDataPartition to create training and test data----- 
set.seed(111) # always set.seed  

trainIndex <- createDataPartition(cat_prediction$will_bite, # vector of outcomes
                                  p = 0.7, 
                                  list = FALSE, 
                                  times = 1)

cat_prediction_train <- cat_prediction[ trainIndex,] # 351 observations
cat_prediction_test  <- cat_prediction[-trainIndex,] # 149 observations   

#-----equivalent method using tidyverse-----  
set.seed(111)

cat_prediction_tv <- cat_prediction %>%
  mutate(id = row_number()) # for anti_join by = "id" to create test

cat_prediction_train_tv <- cat_prediction_tv %>% sample_frac(0.7)
cat_prediction_test_tv  <- anti_join(cat_prediction_tv, cat_prediction_train_tv,
                                     by = 'id')

# compare::compare(cat_prediction_train, cat_prediction_train_tv)
#> FALSE 

# note that these two methods sample different observations despite set.seed(111)   
```

**Note:** There may be times when you will be interested in using [stratified sampling](https://stats.stackexchange.com/questions/250273/benefits-of-stratified-vs-random-sampling-for-generating-training-data-in-classi) to make sure that the representation of a certain demographic is proportionally represented in both the train and test data. This can be done by implementing `group_by` via a `tidyverse` [approach](https://stackoverflow.com/questions/54566428/caret-creating-stratified-data-sets-based-on-several-variables).    

```{r}
#-----doublecheck the train data-----
head(cat_prediction_train) # check values  
str(cat_prediction_train) # check column type  
```


### Obtain random forest friendly train and test data    

Because random forest algorithms handle both numerical categorical and numeric variables, transformations are not needed for predictor variables in `cat_prediction_train` and `cat_prediction_test`. The only modification that we need to perform is to convert the response variable `will_bite` into a factor.      

```{r} 
#-----convert response variable into a factor-----
cat_prediction_train$will_bite <- factor(cat_prediction_train$will_bite)
cat_prediction_test$will_bite <- factor(cat_prediction_test$will_bite)

# str(cat_prediction_train$will_bite)
#> Factor w/ 2 levels "no","yes": 1 1 1 2 1 1 1 1 1 1 ...  
```

```{r}
#-----identify model parameters for tuning-----   
modelLookup(model = "rf")
```

```{r, warning = FALSE, fig.align = "center", out.width = '60%'}
#-----implementing parallel processing-----
# implementing parallel processing  

core_number <- detectCores() - 1  
cl <- makePSOCKcluster(core_number)
registerDoParallel(cl)  

#-----creating the random forest model-----  
# learn how to set.seed when parallel processing  

# enable cross-validation  
control <- trainControl(method='repeatedcv', 
                        number = 10, 
                        repeats = 3,
                        search = 'grid') # hyperparameter tuning enabled   

tunegrid <- expand.grid(mtry = c(1:4))  

rf_model <- train(will_bite ~ .,
                  data = cat_prediction_train,
                  method = "rf",
                  metric = "Accuracy",
                  ntrees = 50,  
                  tuneGrid = tunegrid, 
                  trControl = control)

stopCluster(cl) # stop parallel processing

#-----visualise best random forest model-----
print(rf_model) 
plot(rf_model)
```


### Obtain XGBoost friendly train and test data      

The XGBoost algorithm only handles numerical variables and predictor variables need to be stored as a `DMatrix` object. The response variable also needs to be converted into a numeric vector (i.e. `0` or `1`).    

```{r} 
#-----convert categorical predictor variables into dummy variables-----
categorical_variables <- cat_prediction_train %>%
  select(-will_bite) %>% # remove response variable
  select_if(is.character) %>%
  colnames(.)

# create dummy variables in train data
cat_prediction_train_x <- cat_prediction_train %>%
  select(-will_bite) %>%
  dummy_cols(., 
             select_columns = categorical_variables, 
             remove_selected_columns = T) 

# create dummy variables in test data
cat_prediction_test_x <- cat_prediction_test %>%
  select(-will_bite) %>%
  dummy_cols(., 
             select_columns = categorical_variables, 
             remove_selected_columns = T)  

#-----convert response variables into a numeric factor-----
# create numerical response variable from train data
y_train_xgb <- cat_prediction_train$will_bite 

levels(y_train_xgb) <- list("0" = "no",
                            "1" = "yes")

# create numerical response variable from test data
y_test_xgb <- cat_prediction_test$will_bite 

levels(y_test_xgb) <- list("0" = "no",
                           "1" = "yes")
```

```{r}
#-----identify model parameters for tuning-----   
modelLookup(model = "xgbTree")
```

```{r, fig.align = "center", out.width = '60%'}
#-----creating the XGBoost model-----   
# convert train and test predictor variables into DMatrixes   
x_train_xgb <- as.matrix(cat_prediction_train_x) %>%
  xgb.DMatrix()

x_test_xgb <- as.matrix(cat_prediction_test_x) %>%
  xgb.DMatrix()

set.seed(111)  

control <- trainControl(method='repeatedcv', 
                        number = 10, 
                        repeats = 3,
                        search = 'grid', # hyperparameter tuning enabled
                        allowParallel = F, # xgboost already enables parallel processing
                        returnData = F)     

tunegrid <- expand.grid(nrounds = c(10, 20, 30, 40, 50),
                        max_depth = c(1, 2, 3, 4),
                        colsample_bytree = seq(0.5, 0.9, length.out = 5),
                        eta = 0.1,
                        gamma = 0,
                        min_child_weight = 1,
                        subsample = 1)  

xgboost_model <- train(x_train_xgb, # predictor variables
                       y_train_xgb, # response variable
                       method = "xgbTree",
                       metric = "Accuracy",
                       tuneGrid = tunegrid, 
                       trControl = control)

#-----visualise best XGBoost model-----
print(xgboost_model) 
plot(xgboost_model)
```


### Obtain linear SVM friendly train and test data     

The linear SVM algorithm requires scaled numerical predictor variables. The response variable needs to be converted into a factor.         

```{r}
#-----convert categorical predictor variables into dummy variables-----
categorical_variables <- cat_prediction_train %>%
  select(-will_bite) %>% # remove response variable
  select_if(is.character) %>%
  colnames(.)

# create dummy variables in train data
cat_prediction_train_svm <- cat_prediction_train %>%
  dummy_cols(., 
             select_columns = categorical_variables, 
             remove_selected_columns = T) 

# create dummy variables in test data
cat_prediction_test_svm <- cat_prediction_test %>%
  dummy_cols(., 
             select_columns = categorical_variables, 
             remove_selected_columns = T)  

#-----convert response variable into a factor-----
cat_prediction_train_svm$will_bite <- factor(cat_prediction_train_svm$will_bite)
cat_prediction_test_svm$will_bite <- factor(cat_prediction_test_svm$will_bite)  
```

```{r}
#-----identify model parameters for tuning-----   
modelLookup(model = "svmLinear")
```

```{r, warning = FALSE, fig.align = "center", out.width = '60%'}
#-----implementing parallel processing-----
# implementing parallel processing  

core_number <- detectCores() - 1  
cl <- makePSOCKcluster(core_number)
registerDoParallel(cl)  

#-----creating the linear SVM model----- 
# learn how to set.seed when parallel processing   

control <- trainControl(method='repeatedcv', 
                        number = 10, 
                        repeats = 3,
                        search = 'grid') # hyperparameter tuning enabled   

tunegrid <- expand.grid(C = c(0.1, 0.5, 1.0, 1.5, 2.0))  

svm_model <- train(will_bite ~ ., 
                   cat_prediction_train_svm, # response variable
                   method = "svmLinear",
                   metric = "Accuracy",
                   tuneGrid = tunegrid, 
                   trControl = control,
                   preProcess = c("center", "scale")) # sensitive to outliers  

stopCluster(cl) # stop parallel processing

#-----visualise best linear SVM model-----
print(svm_model) 
plot(svm_model)
```

**Note:** This tutorial only addresses the finicky factor and dummy variable conversions required when testing multiple machine learning algorithms. A complete machine learning workflow will also cover the evaluation of predictions generated in the test data and model-agnostic explainability methods, and is beyond the scope of this tutorial.     


# Other resources   

+ The R4DS guide to wrangling [factors](https://r4ds.had.co.nz/factors.html). 

+ A [tutorial](http://www.cookbook-r.com/Manipulating_data/Renaming_levels_of_a_factor/) on renaming factor levels.  

+ A beginner friendly [tutorial](www.rebeccabarter.com/blog/2017-11-17-caret_tutorial/) on using `caret` by Rebecca Barter.  

+ Notes on [model training and tuning with `caret`](https://topepo.github.io/caret/model-training-and-tuning.html#customizing-the-tuning-process).  

+ Notes on [parallel processing with `caret`](https://topepo.github.io/caret/parallel-processing.html).    